backend: llamacpp gpu
checkpoint: Llama-3.2-1B-Instruct-GGUF
iterations: 1
max_memory_used_gbyte: 0.4925956726074219
output_tokens: 32
prefill_tokens_per_second: 4889.78874584559
prompt_tokens: 128
prompts:
- 'Tell me an extremely long story that starts with the following but goes from there:
  word word word word word word word word word word word word word word word word
  word word word word word word word word '
response_tokens: 32
seconds_to_first_token: 0.026177
token_generation_tokens_per_second: 178.86587853888935
warmup_iterations: 0
